{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_&_RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmCnQq77os44",
        "colab_type": "text"
      },
      "source": [
        "- Đọc dữ liệu từ files, không loại bỏ stop word mà chỉ normalize và loại bỏ những kí tự đặc biệt.\n",
        "\n",
        "- Xây dụng từ điển, loại bỏ các từ xuất hiện trong ít văn bản (cụ thể ở đây loại bỏ những từ có document frequence <=10)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYbTmLIGmvWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import re\n",
        "import os\n",
        "def gen_data_and_vocab():\n",
        "  def collect_data_from(parent_path, newsgroup_list, word_count = 'None'):\n",
        "    data = []\n",
        "    for group_id, newsgroup in enumerate(newsgroup_list):\n",
        "      dir_path = parent_path + '\\\\' + newsgroup +'\\\\'\n",
        "      files = [(filename, dir_path + filename) for filename in os.listdir(dir_path)]\n",
        "      files.sort()\n",
        "      label = group_id\n",
        "      print(\"processing: {}-{}\".format(group_id, newsgroup))\n",
        "      for filename, filepath in files:\n",
        "        with open(filepath) as f:\n",
        "          text = f.read().lower()\n",
        "          words = re.split('\\W+', text)\n",
        "          if word_count == 'None':\n",
        "            for word in words:\n",
        "              word_count[word] += 1\n",
        "          content = ' '.join(words)\n",
        "          assert len(content.splitlines()) == 1\n",
        "          data.append(str(label) + '<fff>' + filename + '<fff>' + content)\n",
        "\n",
        "    return data\n",
        "\n",
        "  word_count = defaultdict(int)\n",
        "  path = \"C:\\\\Users\\\\pl\\\\Downloads\\\\20news-bydate\"\n",
        "  parts = [path +\"\\\\\"+ dir_name for dir_name in os.listdir(path)]\n",
        "  train_path, test_path = (parts[0], parts[1]) if \"train\" in parts[0] else (parts[1], parts[0])\n",
        "  newsgroup_list = [newsgroup for newsgroup in os.listdir(train_path)]\n",
        "  newsgroup_list.sort()\n",
        "  \n",
        "  train_data = collect_data_from(\n",
        "      parent_path = train_path,\n",
        "      newsgroup_list = newsgroup_list,\n",
        "      word_count = word_count\n",
        "    )\n",
        "  vocab = [word for word, freq in word_count.items() if freq > 10]\n",
        "  vocab.sort()\n",
        "  with open(\"C:\\\\Users\\\\pl\\\\Downloads\\\\20news-bydate\\\\vocab-raw.txt\",'w') as f:\n",
        "    f.write('\\n'.join(vocab))\n",
        "  test_data = collect_data_from(\n",
        "      parent_path = test_path,\n",
        "      newsgroup_list = newsgroup_list\n",
        "    )\n",
        "  with open(\"C:\\\\Users\\\\pl\\\\Downloads\\\\20news-bydate\\\\20news-train-raw.txt\",'w') as f:\n",
        "    f.write('\\n'.join(train_data))\n",
        "  with open(\"C:\\\\Users\\\\pl\\\\Downloads\\\\20news-bydate\\\\20news-test-raw.txt\",'w') as f:\n",
        "    f.write('\\n'.join(test_data))\n",
        "\n",
        "gen_data_and_vocab()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMx-ovUwozbl",
        "colab_type": "text"
      },
      "source": [
        "- Mã hóa mỗi từ trong từ điển thành 1 số id tương ứng (từ 2 đến kích thước từ điển - 1), những từ không có trong từ điển sẽ có id là 1.\n",
        "\n",
        "- Mỗi văn bản sẽ được chuyển thành vector có độ dài cố định (độ dài MAX_DOC_LENGTH = 500 từ) các từ trong văn bản sẽ được giữ nguyên vị trí và được thay thế bằng id của nó. \n",
        "\n",
        "- Trong trường hợp số từ trong văn bản nhỏ hơn MAX_DOC_LENGTH sẽ thêm các từ padding có id là 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcXh7yMum3rF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_DOC_LENGTH = 500\n",
        "unknown_ID = 1\n",
        "padding_ID = 0\n",
        "\n",
        "def encode_data(data_path, vocab_path):\n",
        "  with open(vocab_path,encoding='latin-1') as f:\n",
        "    vocab = dict([(word, word_ID + 2) \n",
        "                  for word_ID, word in enumerate(f.read().splitlines())])\n",
        "  with open(data_path,encoding='latin-1') as f :\n",
        "    documents = f.read().splitlines()\n",
        "  encoded_data = []\n",
        "  for document in documents:\n",
        "    label, doc_id, text = document.split('<fff>')\n",
        "    words = text.split()[:MAX_DOC_LENGTH]\n",
        "    sentence_length = len(words)\n",
        "    encoded_text = []\n",
        "    for word in words:\n",
        "      if word in vocab:\n",
        "        encoded_text.append(str(vocab[word]))\n",
        "      else:\n",
        "        encoded_text.append(str(unknown_ID))\n",
        "    if len(words) < MAX_DOC_LENGTH:\n",
        "      num_padding = MAX_DOC_LENGTH - len(words)\n",
        "      for i in range(num_padding):\n",
        "        encoded_text.append(str(padding_ID))\n",
        "    encoded_data.append(str(label) + '<fff>' + str(doc_id) + '<fff>' + \n",
        "                        str(sentence_length) + '<fff>' + ' '.join(encoded_text))\n",
        "\n",
        "  dir_name = '/'.join(data_path.split('/')[:-1])\n",
        "  file_name = '-'.join(data_path.split('/')[-1].split('-')[:-1]) + '-encoded.txt'\n",
        "  with open(dir_name + '/' +file_name, 'w') as f:\n",
        "    f.write('\\n'.join(encoded_data))\n",
        "\n",
        "encode_data(data_path='/content/drive/My Drive/Data_Colab/20news-train-raw.txt',\n",
        "            vocab_path='/content/drive/My Drive/Data_Colab/vocab-raw.txt')\n",
        "encode_data(data_path='/content/drive/My Drive/Data_Colab/20news-test-raw.txt',\n",
        "            vocab_path='/content/drive/My Drive/Data_Colab/vocab-raw.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm0mCVOe0Lq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def load_data(data_path):\n",
        "  with open(data_path, encoding = 'latin1') as f:\n",
        "    d_lines = f.read().splitlines()\n",
        "  data, labels, sentence_lengths = [], [], []\n",
        "  for line in d_lines:\n",
        "    features = line.split('<fff>')\n",
        "    label, doc_id, sentence_len = int(features[0]), int(features[1]), int(features[2])\n",
        "    vector = [int(ID) for ID in features[3].split()]\n",
        "    data.append(vector)\n",
        "    labels.append(label)\n",
        "    sentence_lengths.append(sentence_len)\n",
        "  return torch.tensor(data), torch.tensor(labels), torch.tensor(sentence_lengths)\n",
        "  \n",
        "train_data, train_labels, train_sentence_lengths = load_data(\n",
        "    data_path='/content/drive/My Drive/Data_Colab/20news-train-encoded.txt'\n",
        ")\n",
        "test_data, test_labels, test_sentence_lengths = load_data(\n",
        "    data_path='/content/drive/My Drive/Data_Colab/20news-test-encoded.txt'\n",
        ")\n",
        "with open('/content/drive/My Drive/Data_Colab/vocab-raw.txt', encoding = 'latin1') as f:\n",
        "  vocab_size = len(f.read().splitlines())"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDDaf9q7NwbO",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynYHXvTzrEle",
        "colab_type": "text"
      },
      "source": [
        "- Đầu vào đi qua embedding layer thu được ma trận word embedding\n",
        "- Đầu tiên đi qua convolutional layer với 500 5x300 filters, stride 1\n",
        "- Tiếp theo lấy Maxpool theo cả vector chiều thứ 3 của đầu ra\n",
        "- Dropout với xác suất 0.5 để làm giảm overfit.\n",
        "- Cuối cùng đi qua full connected neural network layer đầu ra là số nhãn lớp để thực hiện dự đoán"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdiBf7VOSEfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "EMBEDDING_SIZE = 300\n",
        "MAX_DOC_LENGTH = 500\n",
        "NUM_CLASSES = 20\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_size, batch_size):\n",
        "    super(CNN, self).__init__()\n",
        "    self._vocab_size = vocab_size\n",
        "    self._embedding_size = embedding_size\n",
        "    self._batch_size = batch_size\n",
        "    self.build_graph()\n",
        "\n",
        "  def build_graph(self):\n",
        "    self._embedding_layer = nn.Embedding(self._vocab_size+2, self._embedding_size)\n",
        "    self._convolutional_layer = nn.Sequential(\n",
        "        nn.Conv2d(1, 500, kernel_size=(5, self._embedding_size)),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self._full_connected_layer = nn.Linear(MAX_DOC_LENGTH, NUM_CLASSES)\n",
        "    self._loss_function = nn.CrossEntropyLoss()\n",
        "    self._dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "  def forward(self, inp):\n",
        "    embeddings = self._embedding_layer(inp).unsqueeze(1)\n",
        "    outputs = self._convolutional_layer(embeddings).squeeze(3)\n",
        "    outputs = F.max_pool1d(outputs, kernel_size=outputs.size(2)).squeeze(2)\n",
        "    outputs = self._dropout((outputs))\n",
        "    outputs = self._full_connected_layer(outputs)\n",
        "    return outputs\n",
        "\n",
        "  def fit(self, train_data, train_labels, max_epochs=50 ,learning_rate=0.01, threshold=1e-3):\n",
        "    data_set = TensorDataset(train_data, train_labels)\n",
        "    data_loader = DataLoader(data_set, batch_size = self._batch_size, shuffle = True)\n",
        "    opt = torch.optim.Adam(params = self.parameters(), lr = learning_rate)\n",
        "    self.train()\n",
        "    last_loss = 0\n",
        "    for epoch in range(max_epochs):\n",
        "      new_loss = 0\n",
        "      for data,labels in data_loader:\n",
        "        opt.zero_grad()\n",
        "        prediced = self.forward(data)\n",
        "        loss = self._loss_function(prediced, labels)\n",
        "        new_loss += loss\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "      new_loss = new_loss / len(data_loader)\n",
        "      print('epoch: {}, loss: {}'.format(epoch, new_loss))\n",
        "      print('test accuracy:',self.predict_and_compute_accuracy(test_data, test_labels))\n",
        "      if abs(last_loss - new_loss) <= threshold:\n",
        "        return\n",
        "      last_loss=new_loss\n",
        "\n",
        "  def predict_and_compute_accuracy(self, test_data, test_labels):\n",
        "    data_set = TensorDataset(test_data, test_labels)\n",
        "    data_loader = DataLoader(data_set, batch_size = self._batch_size, shuffle = False)\n",
        "    num_true_predict = 0\n",
        "    for data, labels in data_loader:\n",
        "      predicted = torch.argmax(self.forward(data), dim = 1)\n",
        "      num_true_predict += sum((predicted == labels).float())\n",
        "    return num_true_predict*100./len(test_data)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0UZ9Nr9qDVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "\n",
        "t=time()\n",
        "cnn = CNN(\n",
        "    vocab_size=vocab_size, \n",
        "    embedding_size=475,  \n",
        "    batch_size=5\n",
        ")\n",
        "cnn.fit(\n",
        "    train_data = train_data,\n",
        "    train_labels = train_labels,\n",
        "    learning_rate=0.01,\n",
        "    threshold=1e-3\n",
        ")\n",
        "print('training time:',time()-t,'s')\n",
        "print('train accuracy:', cnn.predict_and_compute_accuracy(train_data, train_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRrI8PExOCMr",
        "colab_type": "text"
      },
      "source": [
        "# Recurrent Neural Network (RNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCEK2x4OzMpi",
        "colab_type": "text"
      },
      "source": [
        "- Đầu vào đi qua embedding layer thu được ma trận word embedding\n",
        "- Long Short Term Memory layer, lấy trung bình của các hidden state của các thuộc tính không phải padding\n",
        "- Dropout với xác suất 0.5 để làm giảm overfit\n",
        "- Fully connected layer cho đầu ra là số nhãn lớp để chọn ra nhãn lớp có giá trị cao nhất"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuT4TzbLONZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "MAX_DOC_LENGTH = 500\n",
        "NUM_CLASSES = 20\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_size, lstm_size, batch_size):\n",
        "    super().__init__()\n",
        "    self._vocab_size = vocab_size\n",
        "    self._embedding_size = embedding_size\n",
        "    self._lstm_size = lstm_size\n",
        "    self._batch_size = batch_size\n",
        "    self.build_graph()\n",
        "\n",
        "  def build_graph(self):\n",
        "    self._embedding_layer = nn.Embedding(self._vocab_size+2, self._embedding_size)\n",
        "    self._LSTM_layer = nn.LSTM(self._embedding_size, self._lstm_size, batch_first=True)\n",
        "    self._full_connected_layer = nn.Linear(self._lstm_size, NUM_CLASSES)\n",
        "    self._dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "  def forward(self, data, sentence_lengths):\n",
        "    h = torch.zeros(1, data.size(0), self._lstm_size)\n",
        "    c = torch.zeros(1, data.size(0), self._lstm_size)\n",
        "    embeddings = self._embedding_layer(data)  \n",
        "    lstm_outputs, (hidden, cell)  = self._LSTM_layer(embeddings, (h, c))\n",
        "    lstm_outputs = [lstm_outputs[i][:sentence_lengths[i]].mean(0) for i in range(data.size(0))]\n",
        "    lstm_outputs = torch.stack(lstm_outputs)\n",
        "    outputs = self._dropout(lstm_outputs)\n",
        "    outputs = self._full_connected_layer(lstm_outputs)\n",
        "    return outputs\n",
        "\n",
        "  def fit(self, train_data, train_labels, train_sentence_lengths, max_epochs=50, learning_rate=0.01, threshold=1e-3):\n",
        "    data_set = TensorDataset(train_data, train_labels, train_sentence_lengths)\n",
        "    data_loader = DataLoader(data_set, batch_size = self._batch_size, shuffle = True)\n",
        "    opt = torch.optim.Adam(params = self.parameters(), lr = learning_rate)\n",
        "    self._loss_function = nn.CrossEntropyLoss()\n",
        "    self.train()\n",
        "    last_loss = 0\n",
        "    for epoch in range(max_epochs):\n",
        "      new_loss = 0\n",
        "      for data, labels, sentence_lengths in data_loader:\n",
        "        opt.zero_grad()\n",
        "        prediced = self.forward(data, sentence_lengths)\n",
        "        loss = self._loss_function(prediced, labels)\n",
        "        new_loss += loss\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "      new_loss = new_loss / len(data_loader)\n",
        "      #print('epoch: {}, loss: {}'.format(epoch, new_loss))\n",
        "      #print('test accuracy:',self.predict_and_compute_accuracy(test_data, test_labels, test_sentence_lengths))\n",
        "      if abs(last_loss - new_loss) <= threshold:\n",
        "        return\n",
        "      last_loss=new_loss\n",
        "\n",
        "  def predict_and_compute_accuracy(self, test_data, test_labels, test_sentence_lengths):\n",
        "    data_set = TensorDataset(test_data, test_labels, test_sentence_lengths)\n",
        "    data_loader = DataLoader(data_set, batch_size = self._batch_size, shuffle = False)\n",
        "    num_true_predict = 0\n",
        "    for data, labels, sentence_lengths in data_loader:\n",
        "      predicted = torch.argmax(self.forward(data, sentence_lengths), dim = 1)\n",
        "      num_true_predict += sum((predicted == labels).float())\n",
        "    return num_true_predict*100./len(test_data)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtJwiEG7jKXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "\n",
        "t=time()\n",
        "rnn = RNN(\n",
        "    vocab_size=vocab_size, \n",
        "    embedding_size=475, \n",
        "    lstm_size=90, \n",
        "    batch_size=5\n",
        ")\n",
        "rnn.fit(\n",
        "    train_data = train_data,\n",
        "    train_labels = train_labels,\n",
        "    train_sentence_lengths = train_sentence_lengths,\n",
        "    learning_rate=0.01, \n",
        "    threshold=1e-3\n",
        ")\n",
        "print('training time:',time()-t,'s')\n",
        "print('train accuracy:', rnn.predict_and_compute_accuracy(train_data, train_labels, train_sentence_lengths))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9ty6nzkwPk-",
        "colab_type": "text"
      },
      "source": [
        "Cross Validation tìm tham số tối ưu cho RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNGvGQ2cs4xh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a3c9685f-1822-4040-e0c3-0859b0cfa318"
      },
      "source": [
        "def get_the_best_parameter():\n",
        "  \n",
        "  def cross_validation(num_folds,embedding_size, lstm_size, batch_size):\n",
        "    aver_acc = 0\n",
        "    data_set = TensorDataset(train_data, train_labels, train_sentence_lengths)\n",
        "    data_loader = DataLoader(data_set, batch_size = int(train_data.size(0)/num_folds), shuffle = True)\n",
        "    data_loader = list(data_loader)[:num_folds]\n",
        "    for data, labels, sentence_lengths in data_loader:\n",
        "      data_fold_set = TensorDataset(data, labels, sentence_lengths)\n",
        "      data_fold_loader = DataLoader(data_fold_set, batch_size = int(data.size(0)/2), shuffle = True)\n",
        "      data_fold_loader = list(data_fold_loader)\n",
        "      train_data_fold, valid_data_fold = data_fold_loader[0], data_fold_loader[1]\n",
        "      rnn = RNN(\n",
        "          vocab_size=vocab_size, \n",
        "          embedding_size=embedding_size, \n",
        "          lstm_size=lstm_size, \n",
        "          batch_size=batch_size\n",
        "      )\n",
        "      rnn.fit(\n",
        "          train_data = train_data_fold[0],\n",
        "          train_labels = train_data_fold[1],\n",
        "          train_sentence_lengths = train_data_fold[2],\n",
        "          max_epochs=2,\n",
        "          learning_rate=0.01, \n",
        "          threshold=1e-3\n",
        "      )\n",
        "      acc = rnn.predict_and_compute_accuracy(\n",
        "          test_data = valid_data_fold[0], \n",
        "          test_labels = valid_data_fold[1], \n",
        "          test_sentence_lengths = valid_data_fold[2]\n",
        "      )\n",
        "      print(\"acc:\",acc)\n",
        "      aver_acc += acc\n",
        "    print('aver acc:', aver_acc)\n",
        "    return aver_acc/num_folds\n",
        "  \n",
        "  def range_scan(embedding_size_values, lstm_size_values, batch_size_values):\n",
        "    best_embedding_size = 300\n",
        "    max_acc = 0\n",
        "    for current_embedding_size in embedding_size_values:\n",
        "      aver_acc = cross_validation(\n",
        "          num_folds=5, \n",
        "          embedding_size=current_embedding_size,\n",
        "          lstm_size=50, \n",
        "          batch_size=50\n",
        "      )\n",
        "      if aver_acc>max_acc:\n",
        "        best_embedding_size = current_embedding_size\n",
        "        max_acc=aver_acc\n",
        "    print(\"best embedding size:\", best_embedding_size)\n",
        "    best_lstm_size = 50\n",
        "    max_acc = 0\n",
        "    for current_lstm_size in lstm_size_values:\n",
        "      aver_acc = cross_validation(\n",
        "          num_folds=5,\n",
        "          embedding_size=300,\n",
        "          lstm_size=current_lstm_size, \n",
        "          batch_size=50\n",
        "      )\n",
        "      if aver_acc>max_acc:\n",
        "        best_lstm_size = current_lstm_size\n",
        "        max_acc=aver_acc\n",
        "    print(\"best lstm size:\", best_lstm_size)\n",
        "    best_batch_size = 50\n",
        "    max_acc = 0\n",
        "    for current_batch_size in batch_size_values:\n",
        "      aver_acc = cross_validation(\n",
        "          num_folds=5, \n",
        "          embedding_size=300,\n",
        "          lstm_size=50, \n",
        "          batch_size=current_batch_size\n",
        "      )\n",
        "      if aver_acc>max_acc:\n",
        "        best_batch_size = current_batch_size\n",
        "        max_acc=aver_acc\n",
        "    print(\"best batch size:\", best_batch_size)\n",
        "    return best_embedding_size, best_lstm_size, best_batch_size\n",
        "\n",
        "  embedding_size_values = [i*25 for i in range(1,20)]\n",
        "  lstm_size_values = [i*5 for i in range(1,20)]\n",
        "  batch_size_values = [i*5 for i in range(1,20)]\n",
        "  best_embedding_size, best_lstm_size, best_batch_size = range_scan(embedding_size_values, lstm_size_values, batch_size_values)\n",
        "  return best_embedding_size, best_lstm_size, best_batch_size\n",
        "\n",
        "best_embedding_size, best_lstm_size, best_batch_size = get_the_best_parameter()\n",
        "print(\"best embedding size:\", best_embedding_size)\n",
        "print(\"best lstm size:\", best_lstm_size)\n",
        "print(\"best batch size:\", best_batch_size)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc: tensor(12.2900)\n",
            "acc: tensor(11.9363)\n",
            "acc: tensor(10.4332)\n",
            "acc: tensor(14.3236)\n",
            "acc: tensor(11.9363)\n",
            "aver acc: tensor(60.9195)\n",
            "acc: tensor(12.8205)\n",
            "acc: tensor(17.7719)\n",
            "acc: tensor(14.7657)\n",
            "acc: tensor(14.5889)\n",
            "acc: tensor(18.7445)\n",
            "aver acc: tensor(78.6914)\n",
            "acc: tensor(19.3634)\n",
            "acc: tensor(14.7657)\n",
            "acc: tensor(17.9487)\n",
            "acc: tensor(16.2688)\n",
            "acc: tensor(22.1043)\n",
            "aver acc: tensor(90.4509)\n",
            "acc: tensor(23.1653)\n",
            "acc: tensor(18.5676)\n",
            "acc: tensor(22.2812)\n",
            "acc: tensor(21.2202)\n",
            "acc: tensor(24.8453)\n",
            "aver acc: tensor(110.0796)\n",
            "acc: tensor(23.8727)\n",
            "acc: tensor(25.9063)\n",
            "acc: tensor(24.2263)\n",
            "acc: tensor(23.5190)\n",
            "acc: tensor(25.1105)\n",
            "aver acc: tensor(122.6348)\n",
            "acc: tensor(26.3484)\n",
            "acc: tensor(21.3086)\n",
            "acc: tensor(20.4244)\n",
            "acc: tensor(20.2476)\n",
            "acc: tensor(27.6746)\n",
            "aver acc: tensor(116.0035)\n",
            "acc: tensor(24.0495)\n",
            "acc: tensor(25.8179)\n",
            "acc: tensor(29.6198)\n",
            "acc: tensor(23.0769)\n",
            "acc: tensor(22.3696)\n",
            "aver acc: tensor(124.9337)\n",
            "acc: tensor(31.1229)\n",
            "acc: tensor(29.9735)\n",
            "acc: tensor(33.5102)\n",
            "acc: tensor(29.0893)\n",
            "acc: tensor(32.0071)\n",
            "aver acc: tensor(155.7029)\n",
            "acc: tensor(30.6808)\n",
            "acc: tensor(34.3943)\n",
            "acc: tensor(33.2449)\n",
            "acc: tensor(31.5650)\n",
            "acc: tensor(38.4615)\n",
            "aver acc: tensor(168.3466)\n",
            "acc: tensor(35.7206)\n",
            "acc: tensor(34.9248)\n",
            "acc: tensor(31.2113)\n",
            "acc: tensor(33.7754)\n",
            "acc: tensor(35.2785)\n",
            "aver acc: tensor(170.9107)\n",
            "acc: tensor(44.0318)\n",
            "acc: tensor(37.4005)\n",
            "acc: tensor(35.8974)\n",
            "acc: tensor(29.6198)\n",
            "acc: tensor(30.9461)\n",
            "aver acc: tensor(177.8957)\n",
            "acc: tensor(39.1689)\n",
            "acc: tensor(37.9310)\n",
            "acc: tensor(39.5225)\n",
            "acc: tensor(35.0133)\n",
            "acc: tensor(36.0743)\n",
            "aver acc: tensor(187.7100)\n",
            "acc: tensor(38.8152)\n",
            "acc: tensor(33.2449)\n",
            "acc: tensor(43.6782)\n",
            "acc: tensor(40.6720)\n",
            "acc: tensor(33.2449)\n",
            "aver acc: tensor(189.6552)\n",
            "acc: tensor(42.9708)\n",
            "acc: tensor(42.4403)\n",
            "acc: tensor(35.6322)\n",
            "acc: tensor(41.6446)\n",
            "acc: tensor(33.3333)\n",
            "aver acc: tensor(196.0212)\n",
            "acc: tensor(42.7940)\n",
            "acc: tensor(41.5561)\n",
            "acc: tensor(43.1477)\n",
            "acc: tensor(38.9036)\n",
            "acc: tensor(42.8824)\n",
            "aver acc: tensor(209.2838)\n",
            "acc: tensor(44.5623)\n",
            "acc: tensor(37.8426)\n",
            "acc: tensor(44.7392)\n",
            "acc: tensor(41.3793)\n",
            "acc: tensor(40.6720)\n",
            "aver acc: tensor(209.1954)\n",
            "acc: tensor(44.2971)\n",
            "acc: tensor(42.1751)\n",
            "acc: tensor(39.8762)\n",
            "acc: tensor(38.3731)\n",
            "acc: tensor(39.1689)\n",
            "aver acc: tensor(203.8904)\n",
            "acc: tensor(42.3519)\n",
            "acc: tensor(37.3121)\n",
            "acc: tensor(45.0928)\n",
            "acc: tensor(35.1017)\n",
            "acc: tensor(38.8152)\n",
            "aver acc: tensor(198.6737)\n",
            "acc: tensor(37.4889)\n",
            "acc: tensor(47.9222)\n",
            "acc: tensor(40.5836)\n",
            "acc: tensor(44.1202)\n",
            "acc: tensor(43.9434)\n",
            "aver acc: tensor(214.0583)\n",
            "best embedding size: 475\n",
            "acc: tensor(13.6163)\n",
            "acc: tensor(13.2626)\n",
            "acc: tensor(16.7993)\n",
            "acc: tensor(18.2140)\n",
            "acc: tensor(15.2078)\n",
            "aver acc: tensor(77.0999)\n",
            "acc: tensor(24.8453)\n",
            "acc: tensor(22.0159)\n",
            "acc: tensor(18.6561)\n",
            "acc: tensor(18.3024)\n",
            "acc: tensor(32.5376)\n",
            "aver acc: tensor(116.3572)\n",
            "acc: tensor(23.5190)\n",
            "acc: tensor(28.5588)\n",
            "acc: tensor(27.9399)\n",
            "acc: tensor(37.1353)\n",
            "acc: tensor(19.7171)\n",
            "aver acc: tensor(136.8700)\n",
            "acc: tensor(24.8453)\n",
            "acc: tensor(28.8240)\n",
            "acc: tensor(23.7843)\n",
            "acc: tensor(28.4704)\n",
            "acc: tensor(31.3882)\n",
            "aver acc: tensor(137.3121)\n",
            "acc: tensor(35.2785)\n",
            "acc: tensor(33.4217)\n",
            "acc: tensor(24.2263)\n",
            "acc: tensor(37.1353)\n",
            "acc: tensor(28.6472)\n",
            "aver acc: tensor(158.7091)\n",
            "acc: tensor(34.2175)\n",
            "acc: tensor(31.4766)\n",
            "acc: tensor(33.5102)\n",
            "acc: tensor(29.9735)\n",
            "acc: tensor(25.7294)\n",
            "aver acc: tensor(154.9072)\n",
            "acc: tensor(35.8974)\n",
            "acc: tensor(30.8576)\n",
            "acc: tensor(32.4492)\n",
            "acc: tensor(35.3669)\n",
            "acc: tensor(37.2237)\n",
            "aver acc: tensor(171.7949)\n",
            "acc: tensor(31.9187)\n",
            "acc: tensor(27.4094)\n",
            "acc: tensor(35.9859)\n",
            "acc: tensor(29.3546)\n",
            "acc: tensor(34.2175)\n",
            "aver acc: tensor(158.8859)\n",
            "acc: tensor(35.3669)\n",
            "acc: tensor(35.7206)\n",
            "acc: tensor(28.2051)\n",
            "acc: tensor(43.5897)\n",
            "acc: tensor(40.9372)\n",
            "aver acc: tensor(183.8196)\n",
            "acc: tensor(34.7480)\n",
            "acc: tensor(38.6384)\n",
            "acc: tensor(35.0133)\n",
            "acc: tensor(33.1565)\n",
            "acc: tensor(43.3245)\n",
            "aver acc: tensor(184.8806)\n",
            "acc: tensor(35.4553)\n",
            "acc: tensor(39.0805)\n",
            "acc: tensor(38.7268)\n",
            "acc: tensor(36.6932)\n",
            "acc: tensor(32.0955)\n",
            "aver acc: tensor(182.0513)\n",
            "acc: tensor(40.8488)\n",
            "acc: tensor(35.2785)\n",
            "acc: tensor(35.2785)\n",
            "acc: tensor(37.7542)\n",
            "acc: tensor(35.1017)\n",
            "aver acc: tensor(184.2617)\n",
            "acc: tensor(27.4094)\n",
            "acc: tensor(36.2511)\n",
            "acc: tensor(34.4828)\n",
            "acc: tensor(41.9098)\n",
            "acc: tensor(41.4677)\n",
            "aver acc: tensor(181.5208)\n",
            "acc: tensor(36.8700)\n",
            "acc: tensor(44.4739)\n",
            "acc: tensor(41.9098)\n",
            "acc: tensor(38.7268)\n",
            "acc: tensor(33.5986)\n",
            "aver acc: tensor(195.5791)\n",
            "acc: tensor(40.7604)\n",
            "acc: tensor(33.2449)\n",
            "acc: tensor(41.8214)\n",
            "acc: tensor(37.5774)\n",
            "acc: tensor(37.2237)\n",
            "aver acc: tensor(190.6278)\n",
            "acc: tensor(41.8214)\n",
            "acc: tensor(37.1353)\n",
            "acc: tensor(40.9372)\n",
            "acc: tensor(44.4739)\n",
            "acc: tensor(39.3457)\n",
            "aver acc: tensor(203.7135)\n",
            "acc: tensor(40.4951)\n",
            "acc: tensor(38.1963)\n",
            "acc: tensor(34.7480)\n",
            "acc: tensor(37.0469)\n",
            "acc: tensor(44.8276)\n",
            "aver acc: tensor(195.3139)\n",
            "acc: tensor(41.9098)\n",
            "acc: tensor(43.0592)\n",
            "acc: tensor(44.2971)\n",
            "acc: tensor(42.0867)\n",
            "acc: tensor(40.8488)\n",
            "aver acc: tensor(212.2016)\n",
            "acc: tensor(35.1017)\n",
            "acc: tensor(40.1415)\n",
            "acc: tensor(40.6720)\n",
            "acc: tensor(41.1141)\n",
            "acc: tensor(35.8974)\n",
            "aver acc: tensor(192.9266)\n",
            "best lstm size: 90\n",
            "acc: tensor(54.1114)\n",
            "acc: tensor(51.9010)\n",
            "acc: tensor(52.6967)\n",
            "acc: tensor(58.7091)\n",
            "acc: tensor(54.9956)\n",
            "aver acc: tensor(272.4138)\n",
            "best batch size: 5\n",
            "acc: tensor(45.6233)\n",
            "acc: tensor(50.5747)\n",
            "acc: tensor(50.0442)\n",
            "acc: tensor(49.1600)\n",
            "acc: tensor(47.9222)\n",
            "aver acc: tensor(243.3245)\n",
            "best batch size: 5\n",
            "acc: tensor(45.8002)\n",
            "acc: tensor(51.5473)\n",
            "acc: tensor(43.9434)\n",
            "acc: tensor(44.7392)\n",
            "acc: tensor(49.1600)\n",
            "aver acc: tensor(235.1901)\n",
            "best batch size: 5\n",
            "acc: tensor(48.6295)\n",
            "acc: tensor(48.3643)\n",
            "acc: tensor(46.1538)\n",
            "acc: tensor(49.5137)\n",
            "acc: tensor(45.8886)\n",
            "aver acc: tensor(238.5500)\n",
            "best batch size: 5\n",
            "acc: tensor(45.6233)\n",
            "acc: tensor(40.9372)\n",
            "acc: tensor(42.5287)\n",
            "acc: tensor(42.9708)\n",
            "acc: tensor(44.1202)\n",
            "aver acc: tensor(216.1804)\n",
            "best batch size: 5\n",
            "acc: tensor(40.3183)\n",
            "acc: tensor(40.8488)\n",
            "acc: tensor(39.0805)\n",
            "acc: tensor(42.2635)\n",
            "acc: tensor(38.3731)\n",
            "aver acc: tensor(200.8842)\n",
            "best batch size: 5\n",
            "acc: tensor(42.8824)\n",
            "acc: tensor(39.9646)\n",
            "acc: tensor(48.9832)\n",
            "acc: tensor(41.2909)\n",
            "acc: tensor(33.5102)\n",
            "aver acc: tensor(206.6313)\n",
            "best batch size: 5\n",
            "acc: tensor(42.9708)\n",
            "acc: tensor(37.0469)\n",
            "acc: tensor(38.1079)\n",
            "acc: tensor(40.5836)\n",
            "acc: tensor(31.7418)\n",
            "aver acc: tensor(190.4509)\n",
            "best batch size: 5\n",
            "acc: tensor(41.5561)\n",
            "acc: tensor(40.3183)\n",
            "acc: tensor(31.2113)\n",
            "acc: tensor(34.3943)\n",
            "acc: tensor(33.5986)\n",
            "aver acc: tensor(181.0787)\n",
            "best batch size: 5\n",
            "acc: tensor(33.8638)\n",
            "acc: tensor(35.2785)\n",
            "acc: tensor(35.1901)\n",
            "acc: tensor(32.1839)\n",
            "acc: tensor(32.8028)\n",
            "aver acc: tensor(169.3192)\n",
            "best batch size: 5\n",
            "acc: tensor(38.6384)\n",
            "acc: tensor(29.8851)\n",
            "acc: tensor(32.5376)\n",
            "acc: tensor(29.1777)\n",
            "acc: tensor(31.7418)\n",
            "aver acc: tensor(161.9805)\n",
            "best batch size: 5\n",
            "acc: tensor(38.4615)\n",
            "acc: tensor(28.1167)\n",
            "acc: tensor(37.8426)\n",
            "acc: tensor(33.5986)\n",
            "acc: tensor(31.7418)\n",
            "aver acc: tensor(169.7613)\n",
            "best batch size: 5\n",
            "acc: tensor(34.9248)\n",
            "acc: tensor(28.2051)\n",
            "acc: tensor(39.8762)\n",
            "acc: tensor(33.7754)\n",
            "acc: tensor(34.0407)\n",
            "aver acc: tensor(170.8223)\n",
            "best batch size: 5\n",
            "acc: tensor(26.5252)\n",
            "acc: tensor(34.1291)\n",
            "acc: tensor(35.3669)\n",
            "acc: tensor(28.7356)\n",
            "acc: tensor(28.2935)\n",
            "aver acc: tensor(153.0504)\n",
            "best batch size: 5\n",
            "acc: tensor(29.7082)\n",
            "acc: tensor(29.1777)\n",
            "acc: tensor(31.7418)\n",
            "acc: tensor(35.8974)\n",
            "acc: tensor(34.2175)\n",
            "aver acc: tensor(160.7427)\n",
            "best batch size: 5\n",
            "acc: tensor(27.0557)\n",
            "acc: tensor(29.2661)\n",
            "acc: tensor(25.7294)\n",
            "acc: tensor(32.5376)\n",
            "acc: tensor(25.7294)\n",
            "aver acc: tensor(140.3183)\n",
            "best batch size: 5\n",
            "acc: tensor(35.2785)\n",
            "acc: tensor(32.5376)\n",
            "acc: tensor(32.4492)\n",
            "acc: tensor(26.7020)\n",
            "acc: tensor(25.4642)\n",
            "aver acc: tensor(152.4315)\n",
            "best batch size: 5\n",
            "acc: tensor(22.9001)\n",
            "acc: tensor(33.4217)\n",
            "acc: tensor(27.2325)\n",
            "acc: tensor(23.6074)\n",
            "acc: tensor(32.1839)\n",
            "aver acc: tensor(139.3457)\n",
            "best batch size: 5\n",
            "acc: tensor(24.5800)\n",
            "acc: tensor(29.0009)\n",
            "acc: tensor(35.8090)\n",
            "acc: tensor(24.9337)\n",
            "acc: tensor(32.3607)\n",
            "aver acc: tensor(146.6844)\n",
            "best batch size: 5\n",
            "best embedding size: 475\n",
            "best lstm size: 90\n",
            "best batch size: 5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}